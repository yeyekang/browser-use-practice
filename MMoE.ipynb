{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1kas1EzYpcmlUCea2leEoMz0diBaZBaXf",
      "authorship_tag": "ABX9TyOHYzgGm9iISuANkXQut6RS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeyekang/Comparison-of-Multi-task-Models/blob/main/MMoE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# ---------------------------\n",
        "# MMoE Layer\n",
        "# ---------------------------\n",
        "class MMoE(layers.Layer):\n",
        "    def __init__(self, units, num_experts, num_tasks, **kwargs):\n",
        "        super(MMoE, self).__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.num_experts = num_experts\n",
        "        self.num_tasks = num_tasks\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_dim = int(input_shape[-1])\n",
        "\n",
        "        # Experts\n",
        "        self.expert_kernels = self.add_weight(\n",
        "            name=\"expert_kernels\",\n",
        "            shape=(self.num_experts, input_dim, self.units),\n",
        "            initializer=\"glorot_uniform\",\n",
        "            trainable=True,\n",
        "        )\n",
        "        self.expert_bias = self.add_weight(\n",
        "            name=\"expert_bias\",\n",
        "            shape=(self.num_experts, self.units),\n",
        "            initializer=\"zeros\",\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "        # Gates\n",
        "        self.gate_kernels = self.add_weight(\n",
        "            name=\"gate_kernels\",\n",
        "            shape=(self.num_tasks, input_dim, self.num_experts),\n",
        "            initializer=\"glorot_uniform\",\n",
        "            trainable=True,\n",
        "        )\n",
        "        self.gate_bias = self.add_weight(\n",
        "            name=\"gate_bias\",\n",
        "            shape=(self.num_tasks, self.num_experts),\n",
        "            initializer=\"zeros\",\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "        super(MMoE, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Experts: (batch, num_experts, units)\n",
        "        expert_outputs = tf.einsum(\"bi,eiu->beu\", inputs, self.expert_kernels) + self.expert_bias\n",
        "        expert_outputs = tf.nn.relu(expert_outputs)\n",
        "\n",
        "        outputs = []\n",
        "        for i in range(self.num_tasks):\n",
        "            gate_logits = tf.matmul(inputs, self.gate_kernels[i]) + self.gate_bias[i]\n",
        "            gate_softmax = tf.nn.softmax(gate_logits, axis=-1)\n",
        "            # Weighted sum of experts: (batch, units)\n",
        "            weighted_expert_output = tf.einsum(\"beu,be->bu\", expert_outputs, gate_softmax)\n",
        "            outputs.append(weighted_expert_output)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Build Model\n",
        "# ---------------------------\n",
        "def build_mmoe_model(input_dim, num_experts=8, units=16):\n",
        "    inputs = layers.Input(shape=(input_dim,))\n",
        "    mmoe_outputs = MMoE(units=units, num_experts=num_experts, num_tasks=2)(inputs)\n",
        "\n",
        "    # Task 1: income\n",
        "    tower_income = layers.Dense(8, activation=\"relu\")(mmoe_outputs[0])\n",
        "    output_income = layers.Dense(2, activation=\"softmax\", name=\"income\")(tower_income)\n",
        "\n",
        "    # Task 2: marital\n",
        "    tower_marital = layers.Dense(8, activation=\"relu\")(mmoe_outputs[1])\n",
        "    output_marital = layers.Dense(2, activation=\"softmax\", name=\"marital\")(tower_marital)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=[output_income, output_marital])\n",
        "    model.compile(\n",
        "        optimizer=\"adam\",\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics={\"income\": \"accuracy\", \"marital\": \"accuracy\"},\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Training & Evaluation\n",
        "# ---------------------------\n",
        "def evaluate_model(model, x_train, y_train, x_val, y_val, x_test, y_test, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        model.fit(\n",
        "            x_train,\n",
        "            {\"income\": y_train[:, 0], \"marital\": y_train[:, 1]},\n",
        "            validation_data=(x_val, {\"income\": y_val[:, 0], \"marital\": y_val[:, 1]}),\n",
        "            epochs=1,\n",
        "            batch_size=128,\n",
        "            verbose=1,\n",
        "        )\n",
        "\n",
        "        # predict prob for AUC\n",
        "        y_train_pred = model.predict(x_train, verbose=0)\n",
        "        y_val_pred = model.predict(x_val, verbose=0)\n",
        "        y_test_pred = model.predict(x_test, verbose=0)\n",
        "\n",
        "        for i, task in enumerate([\"income\", \"marital\"]):\n",
        "            auc_train = roc_auc_score(y_train[:, i], y_train_pred[i][:, 1])\n",
        "            auc_val = roc_auc_score(y_val[:, i], y_val_pred[i][:, 1])\n",
        "            auc_test = roc_auc_score(y_test[:, i], y_test_pred[i][:, 1])\n",
        "            print(f\"[Epoch {epoch+1}] {task} - AUC: Train={auc_train:.4f}, Val={auc_val:.4f}, Test={auc_test:.4f}\")\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Main\n",
        "# ---------------------------\n",
        "def main():\n",
        "    # TODO: 替换成你真实的数据加载部分\n",
        "    # 这里假设数据已经是 np.array 并且 y 有两列: [income, marital]\n",
        "    x_train = np.random.rand(34189, 98).astype(np.float32)\n",
        "    y_train = np.random.randint(0, 2, size=(34189, 2))\n",
        "    x_val = np.random.rand(7326, 98).astype(np.float32)\n",
        "    y_val = np.random.randint(0, 2, size=(7326, 2))\n",
        "    x_test = np.random.rand(7327, 98).astype(np.float32)\n",
        "    y_test = np.random.randint(0, 2, size=(7327, 2))\n",
        "\n",
        "    print(\"Training data shape:\", x_train.shape)\n",
        "    print(\"Validation data shape:\", x_val.shape)\n",
        "    print(\"Test data shape:\", x_test.shape)\n",
        "\n",
        "    model = build_mmoe_model(input_dim=x_train.shape[1])\n",
        "    evaluate_model(model, x_train, y_train, x_val, y_val, x_test, y_test, epochs=10)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kL21sSR1o4Wb",
        "outputId": "7a88c6a1-96c2-46f7-be4b-3162ee5f8c0d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (34189, 98)\n",
            "Validation data shape: (7326, 98)\n",
            "Test data shape: (7327, 98)\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 17ms/step - income_accuracy: 0.4996 - income_loss: 0.6944 - loss: 1.3877 - marital_accuracy: 0.5050 - marital_loss: 0.6933 - val_income_accuracy: 0.4993 - val_income_loss: 0.6966 - val_loss: 1.3900 - val_marital_accuracy: 0.4999 - val_marital_loss: 0.6932\n",
            "[Epoch 1] income - AUC: Train=0.5246, Val=0.4884, Test=0.5051\n",
            "[Epoch 1] marital - AUC: Train=0.5075, Val=0.4937, Test=0.4917\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - income_accuracy: 0.5065 - income_loss: 0.6934 - loss: 1.3866 - marital_accuracy: 0.5068 - marital_loss: 0.6931 - val_income_accuracy: 0.4993 - val_income_loss: 0.6959 - val_loss: 1.3893 - val_marital_accuracy: 0.4997 - val_marital_loss: 0.6932\n",
            "[Epoch 2] income - AUC: Train=0.5290, Val=0.4851, Test=0.5094\n",
            "[Epoch 2] marital - AUC: Train=0.5037, Val=0.4946, Test=0.4995\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - income_accuracy: 0.5115 - income_loss: 0.6930 - loss: 1.3862 - marital_accuracy: 0.5059 - marital_loss: 0.6931 - val_income_accuracy: 0.4995 - val_income_loss: 0.6960 - val_loss: 1.3894 - val_marital_accuracy: 0.5001 - val_marital_loss: 0.6932\n",
            "[Epoch 3] income - AUC: Train=0.5313, Val=0.4778, Test=0.5068\n",
            "[Epoch 3] marital - AUC: Train=0.5028, Val=0.5004, Test=0.5010\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - income_accuracy: 0.5095 - income_loss: 0.6930 - loss: 1.3862 - marital_accuracy: 0.5047 - marital_loss: 0.6932 - val_income_accuracy: 0.4996 - val_income_loss: 0.6962 - val_loss: 1.3896 - val_marital_accuracy: 0.4999 - val_marital_loss: 0.6932\n",
            "[Epoch 4] income - AUC: Train=0.5317, Val=0.4814, Test=0.5060\n",
            "[Epoch 4] marital - AUC: Train=0.5000, Val=0.5000, Test=0.5000\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - income_accuracy: 0.5159 - income_loss: 0.6927 - loss: 1.3858 - marital_accuracy: 0.5056 - marital_loss: 0.6931 - val_income_accuracy: 0.4986 - val_income_loss: 0.6958 - val_loss: 1.3891 - val_marital_accuracy: 0.4999 - val_marital_loss: 0.6932\n",
            "[Epoch 5] income - AUC: Train=0.5324, Val=0.4817, Test=0.5048\n",
            "[Epoch 5] marital - AUC: Train=0.5002, Val=0.5003, Test=0.4997\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - income_accuracy: 0.5141 - income_loss: 0.6925 - loss: 1.3857 - marital_accuracy: 0.5056 - marital_loss: 0.6931 - val_income_accuracy: 0.5001 - val_income_loss: 0.6964 - val_loss: 1.3898 - val_marital_accuracy: 0.4999 - val_marital_loss: 0.6932\n",
            "[Epoch 6] income - AUC: Train=0.5344, Val=0.4815, Test=0.5054\n",
            "[Epoch 6] marital - AUC: Train=0.5000, Val=0.5000, Test=0.5000\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - income_accuracy: 0.5174 - income_loss: 0.6924 - loss: 1.3855 - marital_accuracy: 0.5056 - marital_loss: 0.6931 - val_income_accuracy: 0.4984 - val_income_loss: 0.6968 - val_loss: 1.3902 - val_marital_accuracy: 0.4999 - val_marital_loss: 0.6932\n",
            "[Epoch 7] income - AUC: Train=0.5355, Val=0.4822, Test=0.5034\n",
            "[Epoch 7] marital - AUC: Train=0.5000, Val=0.5001, Test=0.5000\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - income_accuracy: 0.5199 - income_loss: 0.6922 - loss: 1.3853 - marital_accuracy: 0.5055 - marital_loss: 0.6931 - val_income_accuracy: 0.4992 - val_income_loss: 0.6964 - val_loss: 1.3897 - val_marital_accuracy: 0.4999 - val_marital_loss: 0.6932\n",
            "[Epoch 8] income - AUC: Train=0.5380, Val=0.4831, Test=0.5028\n",
            "[Epoch 8] marital - AUC: Train=0.5003, Val=0.5001, Test=0.5002\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - income_accuracy: 0.5099 - income_loss: 0.6924 - loss: 1.3855 - marital_accuracy: 0.5053 - marital_loss: 0.6932 - val_income_accuracy: 0.5001 - val_income_loss: 0.6987 - val_loss: 1.3921 - val_marital_accuracy: 0.4999 - val_marital_loss: 0.6932\n",
            "[Epoch 9] income - AUC: Train=0.5376, Val=0.4804, Test=0.5032\n",
            "[Epoch 9] marital - AUC: Train=0.5048, Val=0.5013, Test=0.4986\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - income_accuracy: 0.5205 - income_loss: 0.6921 - loss: 1.3852 - marital_accuracy: 0.5046 - marital_loss: 0.6930 - val_income_accuracy: 0.4967 - val_income_loss: 0.6988 - val_loss: 1.3922 - val_marital_accuracy: 0.4999 - val_marital_loss: 0.6932\n",
            "[Epoch 10] income - AUC: Train=0.5395, Val=0.4793, Test=0.5008\n",
            "[Epoch 10] marital - AUC: Train=0.5002, Val=0.5005, Test=0.4999\n"
          ]
        }
      ]
    }
  ]
}